{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "> Chains-based functions for PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pdf.chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.chains import *\n",
    "from langchain_ray.pdf.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def pdf_docs_chain(\n",
    "    chunk_size=200, chunk_overlap=20, verbose=False, input_key=\"pdf_folder\", output_key=\"df\"\n",
    "):\n",
    "    \"Chain for loading and splitting PDFs into Documents.\"\n",
    "    pdf_chain = transform_chain(create_pdf_df, input_key=input_key)\n",
    "    docs_chain = transform_chain(\n",
    "        df_pdf_docs,\n",
    "        transform_kwargs={\"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap},\n",
    "    )\n",
    "    return SimpleSequentialChain(\n",
    "        chains=[pdf_chain, docs_chain],\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_cats_chain(cats_model, input_key=\"df\", output_key=\"df\"):\n",
    "    \"Chain for adding categories to PDF Documents.\"\n",
    "    return transform_chain(\n",
    "        df_docs_cat,\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        transform_kwargs={\"cats_model\": cats_model},\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_ems_chain(ems_model, ems_folder, input_key=\"df\", output_key=\"df\"):\n",
    "    \"Chain for adding embeddings to a PDF Documents DataFrame.\"\n",
    "    transform_chain(\n",
    "        df_docs_ems,\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"ems_folder\": ems_folder,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_faiss_chain(ems_model, index_folder, index_name, input_key=\"df\", output_key=\"df\"):\n",
    "    \"Chain for adding PDF Documents to a FAISS index.\"\n",
    "    return transform_chain(\n",
    "        df_to_faiss,\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"index_folder\": index_folder,\n",
    "            \"index_name\": index_name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_index_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder to store the FAISS index.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_key=\"pdf_folder\",  # The input key for the PDF folder.\n",
    "    output_key=\"df\",  # The output key for the final DataFrame.\n",
    "    chunk_size=200,  # The number of characters per Document.\n",
    "    chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "    docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "    cats_model=None,  # The HuggingFace model to use for categorization.\n",
    "    ems_chain_model=None,  # The SentenceTransformer model to use for chain embeddings.\n",
    "    ems_folder=None,  # The folder to store the embeddings.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain for adding PDF documents to a FAISS index.\n",
    "    It will be divided and distributed into multiple indexes using Ray if there are more documents than `docs_block_size`.\n",
    "    \"\"\"\n",
    "    chain1 = pdf_docs_chain(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, input_key=input_key\n",
    "    )\n",
    "    index_chains = []\n",
    "    if cats_model is not None:\n",
    "        cats_chain = pdf_cats_chain(cats_model)\n",
    "        index_chains.append(cats_chain)\n",
    "    if ems_folder is not None and ems_chain_model is not None:\n",
    "        ems_chain = pdf_ems_chain(ems_chain_model, ems_folder)\n",
    "        index_chains.append(ems_chain)\n",
    "\n",
    "    faiss_chain = pdf_faiss_chain(ems_model, index_folder, index_name)\n",
    "    index_chains.append(faiss_chain)\n",
    "    chain2 = ray_chain(\n",
    "        SimpleSequentialChain(chains=index_chains),\n",
    "        block_size=docs_block_size,\n",
    "        cuda=True,\n",
    "    )\n",
    "    return SimpleSequentialChain(\n",
    "        chains=[chain1, chain2], input_key=input_key, output_key=output_key, verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "def index_query_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder with the FAISS indexes.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_key=\"query\",  # The input key for the query.\n",
    "    output_key=\"search_results\",  # The output key for the search results.\n",
    "    k=2,  # The number of results to return.\n",
    "    block_size=10,  # The number of indexes to process in a single Ray task.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain for querying the FAISS indexes in `index_folder` with `index_name`.\"\n",
    "    q_df_chain = transform_chain(\n",
    "        create_idx_q_df,\n",
    "        input_key=input_key,\n",
    "        transform_kwargs={\"index_folder\": index_folder, \"index_name\": index_name},\n",
    "    )\n",
    "\n",
    "    search_chain = transform_chain(\n",
    "        lambda df: df.apply(df_search_faiss, axis=1, ems_model=ems_model, k=k),\n",
    "    )\n",
    "\n",
    "    res_chain = transform_chain(\n",
    "        lambda df: sorted(flatten_list(df.results), key=lambda x: x[1]),\n",
    "    )\n",
    "\n",
    "    return ray_chain(\n",
    "        SimpleSequentialChain(\n",
    "            chains=[q_df_chain, search_chain, res_chain],\n",
    "            input_key=input_key,\n",
    "            output_key=output_key,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        block_size=block_size,\n",
    "        cuda=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
