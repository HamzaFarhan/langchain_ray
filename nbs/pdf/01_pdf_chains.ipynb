{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "> Chains-based functions for PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pdf.chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.chains import *\n",
    "from langchain_ray.utils import *\n",
    "from langchain_ray.pdf.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def pdf_to_docs_chain(\n",
    "    splitter=None,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    add_start_index=True,\n",
    "    proc=True,\n",
    "    input_variables=[\"path\"],\n",
    "    output_variables=[\"docs\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Chain that returns a list of `Documents` extracted from a PDF path.\n",
    "    The path can be a single PDF path or a list of paths or a directory path.\"\"\"\n",
    "    return transform_chain(\n",
    "        pdf_to_docs,\n",
    "        transform_kwargs=dict(\n",
    "            splitter=splitter,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=separators,\n",
    "            add_start_index=add_start_index,\n",
    "            proc=proc,\n",
    "        ),\n",
    "        vars_kwargs_mapping={input_variables[0]: \"path\"},\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def add_cats_to_docs_chain(\n",
    "    cats_model,\n",
    "    input_variables=[\"docs\"],\n",
    "    output_variables=[\"cat_docs\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain that adds the categories to a list of `Documents` using `cats_model`.\"\n",
    "    return transform_chain(\n",
    "        add_cats_to_docs,\n",
    "        transform_kwargs=dict(cats_model=cats_model),\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        vars_kwargs_mapping={input_variables[0]: \"docs\"},\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "verbose = True\n",
    "cats_model = SetFitModel.from_pretrained(\"HamzaFarhan/PDFSegs\").to(\"cuda:0\")\n",
    "cats_folder = \"/media/hamza/data2/faiss_data/saved_cats\"\n",
    "if os.path.exists(cats_folder):\n",
    "    shutil.rmtree(cats_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "chain1 = pdf_to_docs_chain(\n",
    "    input_variables=[\"path\"], output_variables=[\"docs\"], verbose=verbose\n",
    ")\n",
    "chain2 = add_cats_to_docs_chain(\n",
    "    cats_model=cats_model,\n",
    "    input_variables=[\"docs\"],\n",
    "    output_variables=[\"cat_docs\"],\n",
    "    verbose=verbose,\n",
    ")\n",
    "chain2 = ray_chain(\n",
    "    chain=chain2,\n",
    "    block_size=2,\n",
    "    num_cpus=6,\n",
    "    num_gpus=0.6,\n",
    "    verbose=verbose,\n",
    ")\n",
    "chain3 = docs_to_json_chain(\n",
    "    json_folder=cats_folder,\n",
    "    input_variables=[\"cat_docs\"],\n",
    "    output_variables=[\"json_docs\"],\n",
    "    verbose=verbose,\n",
    ")\n",
    "chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3],\n",
    "    input_variables=[\"path\"],\n",
    "    output_variables=[\"json_docs\"],\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new TransformChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new TransformChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 02:13:21,472\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mâ„¹ Running chain on 3 blocks.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 02:13:23,593\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-08-12 02:13:23,594\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-12 02:13:23,595\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcf8dda4f984cf397221733141c13ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 1:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffd755840bb4f95b9c2bce397d4e477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split Repartition 2:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f900237da0b140da96c23cf083f27c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=122934)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=122934)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=122934)\u001b[0m \u001b[1m> Entering new TransformChain chain...\u001b[0m\n",
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=122934)\u001b[0m \u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new TransformChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "pdf = \"../../resumes_5/\"\n",
    "res = chain(dict(path=pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../../resumes_5/0bedb223-262c-4388-9756-093dd7905428.pdf', 'page': 1, 'start_index': 1327, 'category': 'Work Experience'}\n",
      "* red hat openshift workshop - containers and kubernetes for developer\n",
      "* google cloud fundamentals: core infrastructure\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "doc = res[\"json_docs\"][0][-1]\n",
    "print(doc.metadata)\n",
    "print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
