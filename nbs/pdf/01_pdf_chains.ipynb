{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "> Chains-based functions for PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pdf.chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.chains import *\n",
    "from langchain_ray.pdf.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def pdf_docs_df_chain(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    input_variables=[\"pdf_folder\"],\n",
    "    output_variables=[\"docs_df\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain that takes a PDF folder and returns a DataFrame of Documents.\"\n",
    "    pdf_chain = transform_chain(\n",
    "        create_pdf_df,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=[\"pdfs_df\"],\n",
    "    )\n",
    "    docs_chain = transform_chain(\n",
    "        df_pdf_docs,\n",
    "        input_variables=[\"pdfs_df\"],\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap},\n",
    "    )\n",
    "    return SequentialChain(\n",
    "        chains=[pdf_chain, docs_chain],\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdfs_to_docs_chain(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    input_variables=[\"pdf_folder\"],\n",
    "    output_variables=[\"docs\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain that takes a PDF folder and returns a DataFrame of Documents.\"\n",
    "    pdf_chain = transform_chain(\n",
    "        pdf_files,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=[\"pdfs\"],\n",
    "    )\n",
    "    docs_chain = transform_chain(\n",
    "        pdfs_to_docs,\n",
    "        input_variables=[\"pdfs\"],\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap},\n",
    "    )\n",
    "    return SequentialChain(\n",
    "        chains=[pdf_chain, docs_chain],\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_cats_chain(\n",
    "    cats_model,\n",
    "    input_variables=[\"docs_df\"],\n",
    "    output_variables=[\"cats_df\"],\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and adds categories using a SetFit model.\"\n",
    "    return transform_chain(\n",
    "        df_docs_cat,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\"cats_model\": cats_model},\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_ems_chain(\n",
    "    ems_model,\n",
    "    ems_folder,\n",
    "    input_variables=[\"docs_df\"],\n",
    "    output_variables=[\"ems_df\"],\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and writes embeddings to `ems_folder` using `ems_model`.\"\n",
    "    transform_chain(\n",
    "        df_docs_ems,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"ems_folder\": ems_folder,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def docs_faiss_chain(\n",
    "    ems_model,\n",
    "    index_folder,\n",
    "    index_name,\n",
    "    input_variables=[\"docs_df\"],\n",
    "    output_variables=[\"faiss_df\"],\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and adds them to a FAISS index in `index_folder`.\"\n",
    "    return transform_chain(\n",
    "        df_to_faiss,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"index_folder\": index_folder,\n",
    "            \"index_name\": index_name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def docs_to_faiss_chain(\n",
    "    ems_model,\n",
    "    index_folder,\n",
    "    index_name,\n",
    "    input_variables=[\"docs\"],\n",
    "    output_variables=[\"docs\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and adds them to a FAISS index in `index_folder`.\"\n",
    "    return transform_chain(\n",
    "        docs_to_faiss,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"index_folder\": index_folder,\n",
    "            \"index_name\": index_name,\n",
    "        },\n",
    "        data_kwargs_mapping={input_variables[0]: \"docs\"},\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdfs_to_faiss_chain(\n",
    "    ems_model,  # The model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder to store the FAISS index.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_variables=[\"pdf_folder\"],  # The input key for the PDF folder.\n",
    "    output_variables=[\"docs\"],  # The output key for the final DataFrame.\n",
    "    chunk_size=200,  # The number of characters per Document.\n",
    "    chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "    docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a `pdf_folder` and adds them to FAISS indexes in `index_folder`.\n",
    "    \"\"\"\n",
    "    docs_chain = pdfs_to_docs_chain(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=[\"dc\"],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    faiss_chain = docs_to_faiss_chain(\n",
    "        ems_model=ems_model,\n",
    "        index_folder=index_folder,\n",
    "        index_name=index_name,\n",
    "        input_variables=[\"dc\"],\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    faiss_chain = ray_chain(\n",
    "        faiss_chain,\n",
    "        block_size=docs_block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return SequentialChain(\n",
    "        chains=[docs_chain, faiss_chain],\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def index_query_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder with the FAISS indexes.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_variables=[\"query\", \"k\"],  # The input key for the query.\n",
    "    output_variables=[\"search_results\"],  # The output key for the search results.\n",
    "    block_size=10,  # The number of indexes to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a query and returns the top `k` results from the FAISS indexes in `index_folder`.\n",
    "    If there are more than `block_size` indexes, search will be distributed using Ray.\n",
    "    \"\"\"\n",
    "    q_df_chain = transform_chain(\n",
    "        create_idx_q_df,\n",
    "        input_variables=[\"query\"],\n",
    "        output_variables=[\"q_df\"],\n",
    "        transform_kwargs={\"index_folder\": index_folder, \"index_name\": index_name},\n",
    "    )\n",
    "\n",
    "    def apply_search(df, ems_model, k):\n",
    "        return df.apply(df_search_faiss, axis=1, ems_model=ems_model, k=k)\n",
    "\n",
    "    search_chain = transform_chain(\n",
    "        apply_search,\n",
    "        transform_kwargs={\"ems_model\": ems_model},\n",
    "        input_variables=[\"q_df\", \"k\"],\n",
    "        output_variables=[\"search_df\"],\n",
    "    )\n",
    "\n",
    "    def flatten_res(df, k):\n",
    "        return [sorted(flatten_list(df.results), key=lambda x: x[1])[:k]]\n",
    "\n",
    "    res_chain = transform_chain(\n",
    "        flatten_res,\n",
    "        input_variables=[\"search_df\", \"k\"],\n",
    "        output_variables=output_variables,\n",
    "    )\n",
    "\n",
    "    return ray_chain(\n",
    "        SequentialChain(\n",
    "            chains=[q_df_chain, search_chain, res_chain],\n",
    "            input_variables=input_variables,\n",
    "            output_variables=output_variables,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        block_size=block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "    )\n",
    "\n",
    "\n",
    "def index_query_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder with the FAISS indexes.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_variables=[\"query\", \"k\"],\n",
    "    output_variables=[\"search_results\"],  # The output key for the search results.\n",
    "    block_size=10,  # The number of indexes to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a query and returns the top `k` results from the FAISS indexes in `index_folder`.\n",
    "    If there are more than `block_size` indexes, search will be distributed using Ray.\n",
    "    \"\"\"\n",
    "\n",
    "    index_names_chain = transform_chain(\n",
    "        index_names,\n",
    "        transform_kwargs={\"index_folder\": index_folder, \"index_name\": index_name},\n",
    "        input_variables=[\"k\"],\n",
    "        output_variables=[\"index_names\"],\n",
    "    )\n",
    "\n",
    "    search_faiss_chain = transform_chain(\n",
    "        search_faiss,\n",
    "        transform_kwargs={\"index_folder\": index_folder, \"ems_model\": ems_model},\n",
    "        input_variables=[\"index_names\", \"query\", \"k\"],\n",
    "        output_variables=[\"res\"],\n",
    "    )\n",
    "\n",
    "    search_faiss_chain = ray_chain(\n",
    "        search_faiss_chain, block_size=block_size, num_cpus=num_cpus, num_gpus=num_gpus\n",
    "    )\n",
    "\n",
    "    def flatten_res(res, k):\n",
    "        if is_list(k):\n",
    "            k = k[0]\n",
    "        return [sorted(flatten_list(res), key=lambda x: x[1])[:k]]\n",
    "\n",
    "    res_chain = transform_chain(\n",
    "        flatten_res,\n",
    "        input_variables=[\"res\", \"k\"],\n",
    "        output_variables=output_variables,\n",
    "    )\n",
    "\n",
    "    return SequentialChain(\n",
    "        chains=[index_names_chain, search_faiss_chain, res_chain],\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "\n",
    "\n",
    "# def pdf_faiss_chain2(\n",
    "#     ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "#     index_folder,  # The folder to store the FAISS index.\n",
    "#     index_name,  # The name of the FAISS index.\n",
    "#     input_variables=[\"pdf_folder\"],  # The input key for the PDF folder.\n",
    "#     output_variables=[\"df\"],  # The output key for the final DataFrame.\n",
    "#     chunk_size=200,  # The number of characters per Document.\n",
    "#     chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "#     docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "#     cats_model=None,  # The HuggingFace model to use for categorization.\n",
    "#     ems_chain_model=None,  # The SentenceTransformer model to use for chain embeddings.\n",
    "#     ems_folder=None,  # The folder to store the embeddings.\n",
    "#     verbose=False,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Chain that takes a PDF folder and adds them to FAISS indexes in `index_folder`. With optional categorization and chain embeddings.\n",
    "#     If there are more than `docs_block_size` Documents, it will be divided and distributed into multiple indexes using Ray.\n",
    "#     \"\"\"\n",
    "#     chain1 = pdf_docs_chain(\n",
    "#         chunk_size=chunk_size, chunk_overlap=chunk_overlap, input_variables=input_variables\n",
    "#     )\n",
    "#     index_chains = []\n",
    "#     if cats_model is not None:\n",
    "#         cats_chain = pdf_cats_chain(cats_model)\n",
    "#         index_chains.append(cats_chain)\n",
    "#     if ems_folder is not None and ems_chain_model is not None:\n",
    "#         ems_chain = pdf_ems_chain(ems_chain_model, ems_folder)\n",
    "#         index_chains.append(ems_chain)\n",
    "\n",
    "#     faiss_chain = docs_faiss_chain(\n",
    "#         ems_model, index_folder, index_name, output_variables=output_variables\n",
    "#     )\n",
    "#     index_chains.append(faiss_chain)\n",
    "#     chain2 = ray_chain(\n",
    "#         SequentialChain(chains=index_chains, output_variables=output_variables),\n",
    "#         block_size=docs_block_size,\n",
    "#         cuda=True,\n",
    "#     )\n",
    "#     return SequentialChain(\n",
    "#         chains=[chain1, chain2],\n",
    "#         input_variables=input_variables,\n",
    "#         output_variables=output_variables,\n",
    "#         verbose=verbose\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load our embeddings model using LangChain's `SentenceTransformerEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"HamzaFarhan/PDFSegs\"\n",
    "\n",
    "ems_model = SentenceTransformerEmbeddings(\n",
    "    model_name=model_name, model_kwargs={\"device\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the `index_folder` and `index_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "data_folder = Path(\"/media/hamza/data2/faiss_data/\")\n",
    "index_folder = data_folder / \"saved_indexes\"\n",
    "index_name = \"chain_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "for f in index_folder.glob(f\"{index_name}*\"):\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a chain for creating FAISS index(es).\n",
    "\n",
    "<br>We're using job resumes in our example and we want to split the text into chunks of 3 lines. A job resume typically has 60-80 characters per line, so we set `chunk_size` to 200. So for each PDF, we'll have (number of lines / 3) `Documents`.\n",
    "\n",
    "<br>Also, let's suppose we have thousands of extracted `Documents` and  we want to parallelize the indexing process.\n",
    "<br>That's where `docs_block_size` comes in. It's the number of `Documents` that will be indexed in parallel using `Ray` tasks. Each task will create a separate FAISS index.\n",
    "<br>You can pass the `num_cpus` and `num_gpus` arguments to specify the number of CPUs and GPUs to use for indexing. Those resources will be distributed evenly across the tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "verbose = True\n",
    "\n",
    "faiss_chain = pdfs_to_faiss_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    docs_block_size=200,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the chain on a sample folder of 5 PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "pdf_folder = Path(\"../../resumes_5/\")\n",
    "\n",
    "faiss_docs = faiss_chain(pdf_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Spearheaded complete purchase cycl e at Ruwais Adnoc Project \\nand expedite the materials & services from source to final delivery \\n In-depth knowledge of export control regulations and import', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 1547}),\n",
       " Document(page_content='procedures inMiddle East \\n Resourceful in finalizing the specifications of materials, \\nestablishing quali ty & quantity limits for effective inventory \\ncontrol and reducing wastages', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 1745}),\n",
       " Document(page_content='Skilled in planning and monitoring warehouse operations of \\nreceipt, storage, return of unused stock, inventory control and \\nmonitoring inbound /outbound logistics', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 1932}),\n",
       " Document(page_content=\"Successfully managed procure materials and services of various \\nprojects onshore & offshore in oil and gas sector. \\n \\n \\n \\n \\nAug'11 Dec '19 with Gastech International FZCO, Dubai\", metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 2104}),\n",
       " Document(page_content=\"Procurement & Logistic s Coordinator \\n \\nJan'0 9 Aug'11 with Mushrif National Construction LLC, Abu Dhabi \\nProcurement Specialist\", metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 2295})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "faiss_docs['docs'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were only 148 `Documents`. So Ray was not used. We can lower the `docs_block_size` to force Ray to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "for f in index_folder.glob(f\"{index_name}*\"):\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "faiss_chain2 = pdfs_to_faiss_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    docs_block_size=50, # Changed\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 00:01:10,122\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[38;5;4mâ„¹ Running chain on 3 blocks.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 00:01:12,120\tWARNING dataset.py:253 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "2023-07-10 00:01:12,124\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-07-10 00:01:12,125\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-07-10 00:01:12,125\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95dbb4dbdea43c5bf16af1241ced27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 1:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c74c43d65964a439155c8fb68437aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repartition 2:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079401b0a2fa4c0cab95a7c4c972d618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=121928)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=121928)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=121928)\u001b[0m \u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[2m\u001b[36m(MapBatches(<lambda>) pid=121928)\u001b[0m \u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 00:01:19,126\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-8, stopped daemon 140169780901632)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "\n",
    "faiss_docs2 = faiss_chain2(pdf_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Spearheaded complete purchase cycl e at Ruwais Adnoc Project \\nand expedite the materials & services from source to final delivery \\n In-depth knowledge of export control regulations and import', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 1547}),\n",
       " Document(page_content='procedures inMiddle East \\n Resourceful in finalizing the specifications of materials, \\nestablishing quali ty & quantity limits for effective inventory \\ncontrol and reducing wastages', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 1745}),\n",
       " Document(page_content='Skilled in planning and monitoring warehouse operations of \\nreceipt, storage, return of unused stock, inventory control and \\nmonitoring inbound /outbound logistics', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 1932}),\n",
       " Document(page_content=\"Successfully managed procure materials and services of various \\nprojects onshore & offshore in oil and gas sector. \\n \\n \\n \\n \\nAug'11 Dec '19 with Gastech International FZCO, Dubai\", metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 2104}),\n",
       " Document(page_content=\"Procurement & Logistic s Coordinator \\n \\nJan'0 9 Aug'11 with Mushrif National Construction LLC, Abu Dhabi \\nProcurement Specialist\", metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 2295})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "faiss_docs2['docs'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that simple! We can now use the FAISS indexes to search for similar Documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `index_query_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "query_chain = index_query_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    block_size=10,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "query = \"I got my degree from the University of Toronto\"\n",
    "search_res = query_chain(dict(query=query, k=3))['search_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m Bachelor of Commerce (B. Com) - University of Mumbai 2008 - 2011\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0cf20170-8051-41ba-9060-1a82d43f4289.pdf', 'page': 0, 'start_index': 3474}\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m in 1997 \n",
      " \n",
      " B.A. from Punjab University, Lahore \n",
      "in 1991 \n",
      " \n",
      " \n",
      "CE R T I F I C A T I O N S :\n",
      " \n",
      " \n",
      " CTLP (Certified Trade & Logistics \n",
      "Professional) from Dubai World, \n",
      "Dubai - UAE in 2012\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 356}\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m EDUCATION \n",
      "Cloud Computing for Big Data , Post Graduate Diploma (GPA 3.61 Dean's Honor list) Jan 2019 - Aug 2020 \n",
      "Lambton College, Toronto, ON\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0bedb223-262c-4388-9756-093dd7905428.pdf', 'page': 1, 'start_index': 1002}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "print(\"Search Results:\\n\")\n",
    "for doc in search_res:\n",
    "    print(f\"+{'-'*100}+\")\n",
    "    print()\n",
    "    print_doc(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
