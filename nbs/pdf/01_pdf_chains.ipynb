{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "> Chains-based functions for PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pdf.chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.chains import *\n",
    "from langchain_ray.pdf.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def pdf_docs_chain(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    input_variables=[\"pdf_folder\"],\n",
    "    output_variables=[\"docs_df\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain that takes a PDF folder and returns a DataFrame of Documents.\"\n",
    "    pdf_chain = transform_chain(\n",
    "        create_pdf_df,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=[\"pdfs_df\"],\n",
    "    )\n",
    "    docs_chain = transform_chain(\n",
    "        df_pdf_docs,\n",
    "        input_variables=[\"pdfs_df\"],\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap},\n",
    "    )\n",
    "    return SequentialChain(\n",
    "        chains=[pdf_chain, docs_chain],\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_cats_chain(\n",
    "    cats_model,\n",
    "    input_variables=[\"docs_df\"],\n",
    "    output_variables=[\"cats_df\"],\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and adds categories using a SetFit model.\"\n",
    "    return transform_chain(\n",
    "        df_docs_cat,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\"cats_model\": cats_model},\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_ems_chain(\n",
    "    ems_model,\n",
    "    ems_folder,\n",
    "    input_variables=[\"docs_df\"],\n",
    "    output_variables=[\"ems_df\"],\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and writes embeddings to `ems_folder` using `ems_model`.\"\n",
    "    transform_chain(\n",
    "        df_docs_ems,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"ems_folder\": ems_folder,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def docs_faiss_chain(\n",
    "    ems_model,\n",
    "    index_folder,\n",
    "    index_name,\n",
    "    input_variables=[\"docs_df\"],\n",
    "    output_variables=[\"faiss_df\"],\n",
    "):\n",
    "    \"Chain that takes a DataFrame of Documents and adds them to a FAISS index in `index_folder`.\"\n",
    "    return transform_chain(\n",
    "        df_to_faiss,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"index_folder\": index_folder,\n",
    "            \"index_name\": index_name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_faiss_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder to store the FAISS index.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_variables=[\"pdf_folder\"],  # The input key for the PDF folder.\n",
    "    output_variables=[\"faiss_df\"],  # The output key for the final DataFrame.\n",
    "    chunk_size=200,  # The number of characters per Document.\n",
    "    chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "    docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a `pdf_folder` and adds them to FAISS indexes in `index_folder`.\n",
    "    If there are more than `docs_block_size` Documents, it will be divided and distributed into multiple indexes using Ray.\n",
    "    \"\"\"\n",
    "    docs_chain = pdf_docs_chain(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=[\"docs_df\"],\n",
    "    )\n",
    "    faiss_chain = ray_chain(\n",
    "        docs_faiss_chain(\n",
    "            ems_model,\n",
    "            index_folder,\n",
    "            index_name,\n",
    "            input_variables=[\"docs_df\"],\n",
    "            output_variables=output_variables,\n",
    "        ),\n",
    "        block_size=docs_block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "    )\n",
    "    return SequentialChain(\n",
    "        chains=[docs_chain, faiss_chain],\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def index_query_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder with the FAISS indexes.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_variables=[\"query\"],  # The input key for the query.\n",
    "    output_variables=[\"search_results\"],  # The output key for the search results.\n",
    "    k=2,  # The number of results to return.\n",
    "    block_size=10,  # The number of indexes to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a query and returns the top `k` results from the FAISS indexes in `index_folder`.\n",
    "    If there are more than `block_size` indexes, search will be distributed using Ray.\n",
    "    \"\"\"\n",
    "    q_df_chain = transform_chain(\n",
    "        create_idx_q_df,\n",
    "        input_variables=input_variables,\n",
    "        output_variables=[\"q_df\"],\n",
    "        transform_kwargs={\"index_folder\": index_folder, \"index_name\": index_name},\n",
    "    )\n",
    "\n",
    "    def apply_search(df, ems_model, k):\n",
    "        return df.apply(df_search_faiss, axis=1, ems_model=ems_model, k=k)\n",
    "\n",
    "    search_chain = transform_chain(\n",
    "        partial(apply_search, ems_model=ems_model, k=k),\n",
    "        input_variables=[\"q_df\"],\n",
    "        output_variables=[\"search\"],\n",
    "    )\n",
    "\n",
    "    def flatten_res(df, k):\n",
    "        return [sorted(flatten_list(df.results), key=lambda x: x[1])[:k]]\n",
    "\n",
    "    res_chain = transform_chain(\n",
    "        flatten_res,\n",
    "        transform_kwargs={\"k\": k},\n",
    "        input_variables=[\"search\"],\n",
    "        output_variables=output_variables,\n",
    "    )\n",
    "\n",
    "    return ray_chain(\n",
    "        SequentialChain(\n",
    "            chains=[q_df_chain, search_chain, res_chain],\n",
    "            input_variables=input_variables,\n",
    "            output_variables=output_variables,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        block_size=block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "\n",
    "\n",
    "# def pdf_faiss_chain2(\n",
    "#     ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "#     index_folder,  # The folder to store the FAISS index.\n",
    "#     index_name,  # The name of the FAISS index.\n",
    "#     input_variables=[\"pdf_folder\"],  # The input key for the PDF folder.\n",
    "#     output_variables=[\"df\"],  # The output key for the final DataFrame.\n",
    "#     chunk_size=200,  # The number of characters per Document.\n",
    "#     chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "#     docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "#     cats_model=None,  # The HuggingFace model to use for categorization.\n",
    "#     ems_chain_model=None,  # The SentenceTransformer model to use for chain embeddings.\n",
    "#     ems_folder=None,  # The folder to store the embeddings.\n",
    "#     verbose=False,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Chain that takes a PDF folder and adds them to FAISS indexes in `index_folder`. With optional categorization and chain embeddings.\n",
    "#     If there are more than `docs_block_size` Documents, it will be divided and distributed into multiple indexes using Ray.\n",
    "#     \"\"\"\n",
    "#     chain1 = pdf_docs_chain(\n",
    "#         chunk_size=chunk_size, chunk_overlap=chunk_overlap, input_variables=input_variables\n",
    "#     )\n",
    "#     index_chains = []\n",
    "#     if cats_model is not None:\n",
    "#         cats_chain = pdf_cats_chain(cats_model)\n",
    "#         index_chains.append(cats_chain)\n",
    "#     if ems_folder is not None and ems_chain_model is not None:\n",
    "#         ems_chain = pdf_ems_chain(ems_chain_model, ems_folder)\n",
    "#         index_chains.append(ems_chain)\n",
    "\n",
    "#     faiss_chain = docs_faiss_chain(\n",
    "#         ems_model, index_folder, index_name, output_variables=output_variables\n",
    "#     )\n",
    "#     index_chains.append(faiss_chain)\n",
    "#     chain2 = ray_chain(\n",
    "#         SequentialChain(chains=index_chains, output_variables=output_variables),\n",
    "#         block_size=docs_block_size,\n",
    "#         cuda=True,\n",
    "#     )\n",
    "#     return SequentialChain(\n",
    "#         chains=[chain1, chain2],\n",
    "#         input_variables=input_variables,\n",
    "#         output_variables=output_variables,\n",
    "#         verbose=verbose\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load our embeddings model using LangChain's `SentenceTransformerEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"HamzaFarhan/PDFSegs\"\n",
    "\n",
    "ems_model = SentenceTransformerEmbeddings(\n",
    "    model_name=model_name, model_kwargs={\"device\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the `index_folder` and `index_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "data_folder = Path(\"/media/hamza/data2/faiss_data/\")\n",
    "index_folder = data_folder / \"saved_indexes\"\n",
    "index_name = \"chain_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "for f in index_folder.glob(f\"{index_name}*\"):\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a chain for creating FAISS index(es).\n",
    "\n",
    "<br>We're using job resumes in our example and we want to split the text into chunks of 3 lines. A job resume typically has 60-80 characters per line, so we set `chunk_size` to 200. So for each PDF, we'll have (number of lines / 3) `Documents`.\n",
    "\n",
    "<br>Also, let's suppose we have thousands of extracted `Documents` and  we want to parallelize the indexing process.\n",
    "<br>That's where `docs_block_size` comes in. It's the number of `Documents` that will be indexed in parallel using `Ray` tasks. Each task will create a separate FAISS index.\n",
    "<br>You can pass the `num_cpus` and `num_gpus` arguments to specify the number of CPUs and GPUs to use for indexing. Those resources will be distributed evenly across the tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "verbose = True\n",
    "\n",
    "faiss_chain = pdf_faiss_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    docs_block_size=1500,\n",
    "    num_cpus=4,\n",
    "    num_gpus=0.4,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the chain on a sample folder of 5 PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "pdf_folder = Path(\"../../resumes_5/\")\n",
    "\n",
    "faiss_df = faiss_chain.run(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain returned a DataFrame with the extracted `Documents`.\n",
    "<br>Let's look at one of the extracted `Documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPage_Content:\u001b[0m both structured and unstructured datasets.\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0cf20170-8051-41ba-9060-1a82d43f4289.pdf', 'page': 0, 'start_index': 171}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "doc = faiss_df.iloc[1].doc\n",
    "print_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "print(len(faiss_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were only 148 `Documents`. So Ray was not used. We can lower the `docs_block_size` to force Ray to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "for f in index_folder.glob(f\"{index_name}*\"):\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "faiss_chain2 = pdf_faiss_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    docs_block_size=50,  # Changed\n",
    "    num_cpus=4,\n",
    "    num_gpus=0.4,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 08:20:11,075\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[38;5;4mâ„¹ Running chain on 3 blocks.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 08:20:13,225\tWARNING dataset.py:253 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "2023-07-09 08:20:13,229\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-07-09 08:20:13,230\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-07-09 08:20:13,230\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dd036ead82486b9f9dac73bbe12630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 1:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518026e0f6334c2587725a8e4d74d816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repartition 2:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ac4e8090ab4688a790e74bd6e791f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 08:20:20,199\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-8, stopped daemon 140108804122368)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "faiss_df2 = faiss_chain2.run(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that simple! We can now use the FAISS indexes to search for similar Documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `index_query_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "query_chain = index_query_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    k=2,\n",
    "    block_size=10,\n",
    "    num_cpus=4,\n",
    "    num_gpus=0.4,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "query = \"I got my degree from the University of Toronto\"\n",
    "search_res = query_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m Bachelor of Commerce (B. Com) - University of Mumbai 2008 - 2011\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0cf20170-8051-41ba-9060-1a82d43f4289.pdf', 'page': 0, 'start_index': 3474}\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m in 1997 \n",
      " \n",
      " B.A. from Punjab University, Lahore \n",
      "in 1991 \n",
      " \n",
      " \n",
      "CE R T I F I C A T I O N S :\n",
      " \n",
      " \n",
      " CTLP (Certified Trade & Logistics \n",
      "Professional) from Dubai World, \n",
      "Dubai - UAE in 2012\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 356}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "print(\"Search Results:\\n\")\n",
    "for doc in search_res:\n",
    "    print(f\"+{'-'*100}+\")\n",
    "    print()\n",
    "    print_doc(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-09 08:20:21,035 E 792600 792612] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-09_08-20-09_379579_792383 is over 95% full, available space: 24836288512; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-09 08:20:31,045 E 792600 792612] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-09_08-20-09_379579_792383 is over 95% full, available space: 24836182016; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-09 08:20:41,059 E 792600 792612] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-09_08-20-09_379579_792383 is over 95% full, available space: 24836149248; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-09 08:20:51,071 E 792600 792612] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-09_08-20-09_379579_792383 is over 95% full, available space: 24835907584; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-09 08:21:01,081 E 792600 792612] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-09_08-20-09_379579_792383 is over 95% full, available space: 24835723264; capacity: 502392610816. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
