{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "> Chains-based functions for PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pdf.chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.chains import *\n",
    "from langchain_ray.pdf.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def pdf_docs_chain(\n",
    "    chunk_size=200, chunk_overlap=20, verbose=False, input_key=\"pdf_folder\", output_key=\"df\"\n",
    "):\n",
    "    \"Chain that takes a PDF folder and returns a DataFrame of Documents.\"\n",
    "    pdf_chain = transform_chain(create_pdf_df, input_key=input_key)\n",
    "    docs_chain = transform_chain(\n",
    "        df_pdf_docs,\n",
    "        transform_kwargs={\"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap},\n",
    "    )\n",
    "    return SimpleSequentialChain(\n",
    "        chains=[pdf_chain, docs_chain],\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_cats_chain(cats_model, input_key=\"df\", output_key=\"df\"):\n",
    "    \"Chain that takes a DataFrame of Documents and adds categories using a SetFit model.\"\n",
    "    return transform_chain(\n",
    "        df_docs_cat,\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        transform_kwargs={\"cats_model\": cats_model},\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_ems_chain(ems_model, ems_folder, input_key=\"df\", output_key=\"df\"):\n",
    "    \"Chain that takes a DataFrame of Documents and writes embeddings to `ems_folder` using `ems_model`.\"\n",
    "    transform_chain(\n",
    "        df_docs_ems,\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"ems_folder\": ems_folder,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def docs_faiss_chain(ems_model, index_folder, index_name, input_key=\"df\", output_key=\"df\"):\n",
    "    \"Chain that takes a DataFrame of Documents and adds them to a FAISS index in `index_folder`.\"\n",
    "    return transform_chain(\n",
    "        df_to_faiss,\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        transform_kwargs={\n",
    "            \"ems_model\": ems_model,\n",
    "            \"index_folder\": index_folder,\n",
    "            \"index_name\": index_name,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_faiss_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder to store the FAISS index.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_key=\"pdf_folder\",  # The input key for the PDF folder.\n",
    "    output_key=\"df\",  # The output key for the final DataFrame.\n",
    "    chunk_size=200,  # The number of characters per Document.\n",
    "    chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "    docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a `pdf_folder` and adds them to FAISS indexes in `index_folder`.\n",
    "    If there are more than `docs_block_size` Documents, it will be divided and distributed into multiple indexes using Ray.\n",
    "    \"\"\"\n",
    "    docs_chain = pdf_docs_chain(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, input_key=input_key\n",
    "    )\n",
    "    faiss_chain = ray_chain(\n",
    "        docs_faiss_chain(ems_model, index_folder, index_name),\n",
    "        block_size=docs_block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "    )\n",
    "    return SimpleSequentialChain(\n",
    "        chains=[docs_chain, faiss_chain],\n",
    "        input_key=input_key,\n",
    "        output_key=output_key,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def index_query_chain(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder with the FAISS indexes.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_key=\"query\",  # The input key for the query.\n",
    "    output_key=\"search_results\",  # The output key for the search results.\n",
    "    k=2,  # The number of results to return.\n",
    "    block_size=10,  # The number of indexes to process in a single Ray task.\n",
    "    num_cpus=12,  # The number of CPUs to use for Ray.\n",
    "    num_gpus=1,  # The number of GPUs to use for Ray.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a query and returns the top `k` results from the FAISS indexes in `index_folder`.\n",
    "    If there are more than `block_size` indexes, search will be distributed using Ray.\n",
    "    \"\"\"\n",
    "    q_df_chain = transform_chain(\n",
    "        create_idx_q_df,\n",
    "        input_key=input_key,\n",
    "        transform_kwargs={\"index_folder\": index_folder, \"index_name\": index_name},\n",
    "    )\n",
    "\n",
    "    search_chain = transform_chain(\n",
    "        lambda df: df.apply(df_search_faiss, axis=1, ems_model=ems_model, k=k),\n",
    "    )\n",
    "\n",
    "    res_chain = transform_chain(\n",
    "        lambda df: sorted(flatten_list(df.results), key=lambda x: x[1])[:k],\n",
    "    )\n",
    "\n",
    "    return ray_chain(\n",
    "        SimpleSequentialChain(\n",
    "            chains=[q_df_chain, search_chain, res_chain],\n",
    "            input_key=input_key,\n",
    "            output_key=output_key,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        block_size=block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def pdf_faiss_chain2(\n",
    "    ems_model,  # The SentenceTransformer model to use for vectorestore embeddings.\n",
    "    index_folder,  # The folder to store the FAISS index.\n",
    "    index_name,  # The name of the FAISS index.\n",
    "    input_key=\"pdf_folder\",  # The input key for the PDF folder.\n",
    "    output_key=\"df\",  # The output key for the final DataFrame.\n",
    "    chunk_size=200,  # The number of characters per Document.\n",
    "    chunk_overlap=20,  # The number of characters to overlap between Documents.\n",
    "    docs_block_size=1500,  # The number of Documents to process in a single Ray task.\n",
    "    cats_model=None,  # The HuggingFace model to use for categorization.\n",
    "    ems_chain_model=None,  # The SentenceTransformer model to use for chain embeddings.\n",
    "    ems_folder=None,  # The folder to store the embeddings.\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chain that takes a PDF folder and adds them to FAISS indexes in `index_folder`. With optional categorization and chain embeddings.\n",
    "    If there are more than `docs_block_size` Documents, it will be divided and distributed into multiple indexes using Ray.\n",
    "    \"\"\"\n",
    "    chain1 = pdf_docs_chain(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, input_key=input_key\n",
    "    )\n",
    "    index_chains = []\n",
    "    if cats_model is not None:\n",
    "        cats_chain = pdf_cats_chain(cats_model)\n",
    "        index_chains.append(cats_chain)\n",
    "    if ems_folder is not None and ems_chain_model is not None:\n",
    "        ems_chain = pdf_ems_chain(ems_chain_model, ems_folder)\n",
    "        index_chains.append(ems_chain)\n",
    "\n",
    "    faiss_chain = docs_faiss_chain(ems_model, index_folder, index_name)\n",
    "    index_chains.append(faiss_chain)\n",
    "    chain2 = ray_chain(\n",
    "        SimpleSequentialChain(chains=index_chains),\n",
    "        block_size=docs_block_size,\n",
    "        cuda=True,\n",
    "    )\n",
    "    return SimpleSequentialChain(\n",
    "        chains=[chain1, chain2], input_key=input_key, output_key=output_key, verbose=verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load our embeddings model using LangChain's `SentenceTransformerEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"HamzaFarhan/PDFSegs\"\n",
    "\n",
    "ems_model = SentenceTransformerEmbeddings(\n",
    "    model_name=model_name, model_kwargs={\"device\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the `index_folder` and `index_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "data_folder = Path(\"/media/hamza/data2/faiss_data/\")\n",
    "index_folder = data_folder / \"saved_indexes\"\n",
    "index_name = \"chain_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "for f in index_folder.glob(f\"{index_name}*\"):\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a chain for creating FAISS index(es).\n",
    "\n",
    "<br>We're using job resumes in our example and we want to split the text into chunks of 3 lines. A job resume typically has 60-80 characters per line, so we set `chunk_size` to 200. So for each PDF, we'll have (number of lines / 3) `Documents`.\n",
    "\n",
    "<br>Also, let's suppose we have thousands of extracted `Documents` and  we want to parallelize the indexing process.\n",
    "<br>That's where `docs_block_size` comes in. It's the number of `Documents` that will be indexed in parallel using `Ray` tasks. Each task will create a separate FAISS index.\n",
    "<br>You can pass the `num_cpus` and `num_gpus` arguments to specify the number of CPUs and GPUs to use for indexing. Those resources will be distributed evenly across the tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "verbose = True\n",
    "\n",
    "faiss_chain = pdf_faiss_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    docs_block_size=1500,\n",
    "    num_cpus=4,\n",
    "    num_gpus=0.4,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the chain on a sample folder of 5 PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m                                                   doc\n",
      "0    page_content='Kirtan Shah (647) 997-9805 || ki...\n",
      "1    page_content='both structured and unstructured...\n",
      "2    page_content='Well-versed in database design &...\n",
      "3    page_content='Built automated scripts to verif...\n",
      "4    page_content='Databases & Libraries: Snowﬂake,...\n",
      "..                                                 ...\n",
      "143  page_content='specifications. \\n Developed sec...\n",
      "144  page_content='Developed the front end using HT...\n",
      "145  page_content='Designed the databases and creat...\n",
      "146  page_content='SQL Server. \\n Configure Java se...\n",
      "147  page_content='Jawaharlal Nehru Technological U...\n",
      "\n",
      "[148 rows x 1 columns]\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m                                                   doc\n",
      "0    page_content='Kirtan Shah (647) 997-9805 || ki...\n",
      "1    page_content='both structured and unstructured...\n",
      "2    page_content='Well-versed in database design &...\n",
      "3    page_content='Built automated scripts to verif...\n",
      "4    page_content='Databases & Libraries: Snowﬂake,...\n",
      "..                                                 ...\n",
      "143  page_content='specifications. \\n Developed sec...\n",
      "144  page_content='Developed the front end using HT...\n",
      "145  page_content='Designed the databases and creat...\n",
      "146  page_content='SQL Server. \\n Configure Java se...\n",
      "147  page_content='Jawaharlal Nehru Technological U...\n",
      "\n",
      "[148 rows x 1 columns]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "pdf_folder = Path(\"../../resumes_5/\")\n",
    "\n",
    "faiss_df = faiss_chain.run(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain returned a DataFrame with the extracted `Documents`.\n",
    "<br>Let's look at one of the extracted `Documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPage_Content:\u001b[0m both structured and unstructured datasets.\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0cf20170-8051-41ba-9060-1a82d43f4289.pdf', 'page': 0, 'start_index': 171}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "doc = faiss_df.iloc[1].doc\n",
    "print_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "print(len(faiss_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were only 148 `Documents`. So Ray was not used. We can lower the `docs_block_size` to force Ray to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "for f in index_folder.glob(f\"{index_name}*\"):\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "faiss_chain2 = pdf_faiss_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    docs_block_size=50,  # Changed\n",
    "    num_cpus=4,\n",
    "    num_gpus=0.4,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m                                                   doc\n",
      "0    page_content='Kirtan Shah (647) 997-9805 || ki...\n",
      "1    page_content='both structured and unstructured...\n",
      "2    page_content='Well-versed in database design &...\n",
      "3    page_content='Built automated scripts to verif...\n",
      "4    page_content='Databases & Libraries: Snowﬂake,...\n",
      "..                                                 ...\n",
      "143  page_content='specifications. \\n Developed sec...\n",
      "144  page_content='Developed the front end using HT...\n",
      "145  page_content='Designed the databases and creat...\n",
      "146  page_content='SQL Server. \\n Configure Java se...\n",
      "147  page_content='Jawaharlal Nehru Technological U...\n",
      "\n",
      "[148 rows x 1 columns]\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 22:46:12,379\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[38;5;4mℹ Running chain on 3 blocks.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 22:46:14,349\tWARNING dataset.py:253 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "2023-07-07 22:46:14,353\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-07-07 22:46:14,354\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-07-07 22:46:14,354\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736eba94e4744cb6a4418da79598fb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 1:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073a0f18c2e64717b65859acd339b91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repartition 2:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9face15ea53b4268925a90f8700bd3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 22:46:21,307\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-8, stopped daemon 140531103418112)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m                                                   doc\n",
      "0    page_content='Kirtan Shah (647) 997-9805 || ki...\n",
      "1    page_content='both structured and unstructured...\n",
      "2    page_content='Well-versed in database design &...\n",
      "3    page_content='Built automated scripts to verif...\n",
      "4    page_content='Databases & Libraries: Snowﬂake,...\n",
      "..                                                 ...\n",
      "143  page_content='specifications. \\n Developed sec...\n",
      "144  page_content='Developed the front end using HT...\n",
      "145  page_content='Designed the databases and creat...\n",
      "146  page_content='SQL Server. \\n Configure Java se...\n",
      "147  page_content='Jawaharlal Nehru Technological U...\n",
      "\n",
      "[148 rows x 1 columns]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "# | output: false\n",
    "\n",
    "\n",
    "faiss_df2 = faiss_chain2.run(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that simple! We can now use the FAISS indexes to search for similar Documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `index_query_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "query_chain = index_query_chain(\n",
    "    ems_model=ems_model,\n",
    "    index_folder=index_folder,\n",
    "    index_name=index_name,\n",
    "    k=2,\n",
    "    block_size=10,\n",
    "    num_cpus=4,\n",
    "    num_gpus=0.4,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m                                  index_folder     index_name  \\\n",
      "0  /media/hamza/data2/faiss_data/saved_indexes  chain_index_2   \n",
      "1  /media/hamza/data2/faiss_data/saved_indexes    chain_index   \n",
      "2  /media/hamza/data2/faiss_data/saved_indexes  chain_index_1   \n",
      "\n",
      "                                            query  \n",
      "0  I got my degree from the University of Toronto  \n",
      "1  I got my degree from the University of Toronto  \n",
      "2  I got my degree from the University of Toronto  \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m                                  index_folder     index_name  \\\n",
      "0  /media/hamza/data2/faiss_data/saved_indexes  chain_index_2   \n",
      "1  /media/hamza/data2/faiss_data/saved_indexes    chain_index   \n",
      "2  /media/hamza/data2/faiss_data/saved_indexes  chain_index_1   \n",
      "\n",
      "                                            query  \\\n",
      "0  I got my degree from the University of Toronto   \n",
      "1  I got my degree from the University of Toronto   \n",
      "2  I got my degree from the University of Toronto   \n",
      "\n",
      "                                             results  \n",
      "0  [(page_content=\"EDUCATION \\nCloud Computing fo...  \n",
      "1  [(page_content='learning new technologies. \\n ...  \n",
      "2  [(page_content='Bachelor of Commerce (B. Com) ...  \u001b[0m\n",
      "\u001b[38;5;200m\u001b[1;3m[(Document(page_content='Bachelor of Commerce (B. Com) - University of Mumbai 2008 - 2011', metadata={'source': '../../resumes_5/0cf20170-8051-41ba-9060-1a82d43f4289.pdf', 'page': 0, 'start_index': 3474}), 0.041394908), (Document(page_content='in 1997 \\n \\n B.A. from Punjab University, Lahore \\nin 1991 \\n \\n \\nCE R T I F I C A T I O N S :\\n \\n \\n CTLP (Certified Trade & Logistics \\nProfessional) from Dubai World, \\nDubai - UAE in 2012', metadata={'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 356}), 0.055728905)]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "query = \"I got my degree from the University of Toronto\"\n",
    "search_res = query_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m Bachelor of Commerce (B. Com) - University of Mumbai 2008 - 2011\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0cf20170-8051-41ba-9060-1a82d43f4289.pdf', 'page': 0, 'start_index': 3474}\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[1mPage_Content:\u001b[0m in 1997 \n",
      " \n",
      " B.A. from Punjab University, Lahore \n",
      "in 1991 \n",
      " \n",
      " \n",
      "CE R T I F I C A T I O N S :\n",
      " \n",
      " \n",
      " CTLP (Certified Trade & Logistics \n",
      "Professional) from Dubai World, \n",
      "Dubai - UAE in 2012\n",
      "\n",
      "\u001b[1mMetadata:\u001b[0m {'source': '../../resumes_5/0f479ee8-5fd9-4f55-b254-5e8feef08038.pdf', 'page': 0, 'start_index': 356}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "print(\"Search Results:\\n\")\n",
    "for doc in search_res:\n",
    "    print(f\"+{'-'*100}+\")\n",
    "    print()\n",
    "    print_doc(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-07 22:46:22,344 E 499132 499144] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-07_22-46-10_723096_498908 is over 95% full, available space: 23693135872; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-07 22:46:32,356 E 499132 499144] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-07_22-46-10_723096_498908 is over 95% full, available space: 23692996608; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-07 22:46:42,367 E 499132 499144] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-07_22-46-10_723096_498908 is over 95% full, available space: 23692918784; capacity: 502392610816. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-07 22:46:52,380 E 499132 499144] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-07-07_22-46-10_723096_498908 is over 95% full, available space: 23692767232; capacity: 502392610816. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
