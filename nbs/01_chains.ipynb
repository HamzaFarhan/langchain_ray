{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "> Functions for creating and manipulating Chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from dreamai.imports import *\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def chainfn_input(data, tfm, tfm_kwargs={}, input_variables=[\"df\"]):\n",
    "    try:\n",
    "        for k in input_variables:\n",
    "            tfm_kwargs[k] = data.get(k, None)\n",
    "        fn_args = inspect.signature(tfm).parameters\n",
    "        pos_args = [k for k in fn_args if fn_args[k].default == inspect._empty]\n",
    "        for pk, ik in zip(pos_args, input_variables):\n",
    "            if pk != \"kwargs\":\n",
    "                tfm_kwargs[pk] = tfm_kwargs.pop(ik)\n",
    "        if \"kwargs\" not in fn_args:\n",
    "            tfm_kwargs = {k: v for k, v in tfm_kwargs.items() if k in fn_args.keys()}\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error in chainfn_input: {e}\", spaced=True)\n",
    "    return tfm_kwargs\n",
    "\n",
    "\n",
    "# def chain_fn(data, tfm, tfm_kwargs={}, input_variables=[\"df\"], output_variables=[\"df\"]):\n",
    "#     tfm_kwargs = chainfn_input(data, tfm, tfm_kwargs, input_variables)\n",
    "#     # print(f'\\n\\nTFM: {tfm}\\n\\n')\n",
    "#     # print(f'\\n\\nTFM KWARGS: {tfm_kwargs.keys()}\\n\\n')\n",
    "#     fn_res = tfm(**tfm_kwargs)\n",
    "#     return chainfn_output(fn_res, output_variables)\n",
    "\n",
    "\n",
    "def chainfn_output(fn_res, output_variables=[\"df\"]):\n",
    "    if not list_or_tuple(fn_res):\n",
    "        fn_res = [fn_res]\n",
    "    if len(fn_res) > len(output_variables):\n",
    "        fn_res = [fn_res]\n",
    "    return {k: r for k, r in zip(output_variables, fn_res)}\n",
    "\n",
    "\n",
    "def chain_fn_args(data, tfm, tfm_kwargs={}, vars_kwargs_mapping={}):\n",
    "    try:\n",
    "        tfm_kwargs = {**data, **tfm_kwargs}\n",
    "        for k, v in vars_kwargs_mapping.items():\n",
    "            tfm_kwargs[v] = tfm_kwargs.pop(k)\n",
    "        fn_args = inspect.signature(tfm).parameters\n",
    "        if \"kwargs\" not in fn_args:\n",
    "            tfm_kwargs = {k: v for k, v in tfm_kwargs.items() if k in fn_args.keys()}\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error in chain_fn_args: {e}\", spaced=True)\n",
    "    return tfm_kwargs\n",
    "\n",
    "\n",
    "def chain_fn(data, tfm, tfm_kwargs={}, vars_kwargs_mapping={}, output_variables=[\"df\"]):\n",
    "    tfm_kwargs = chain_fn_args(data, tfm, tfm_kwargs, vars_kwargs_mapping)\n",
    "    # print(f\"\\n\\nTFM: {tfm}\\n\\n\")\n",
    "    # print(f\"\\n\\nTFM KWARGS: {tfm_kwargs.keys()}\\n\\n\")\n",
    "    fn_res = tfm(**tfm_kwargs)\n",
    "    fn_res = {**data, **chainfn_output(fn_res, output_variables)}\n",
    "    # print(f\"\\n\\nFN RES: {fn_res}\\n\\n\")\n",
    "    return fn_res\n",
    "\n",
    "\n",
    "def transform_chain(\n",
    "    transform,\n",
    "    transform_kwargs={},\n",
    "    vars_kwargs_mapping={},  # For mapping `input_variables` to `transform_kwargs`\n",
    "    input_variables=[\"df\"],\n",
    "    output_variables=[\"df\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Return a `transform` function wrapped in a `TransformChain`.\"\n",
    "    return TransformChain(\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform=partial(\n",
    "            chain_fn,\n",
    "            tfm=transform,\n",
    "            tfm_kwargs=transform_kwargs,\n",
    "            vars_kwargs_mapping=vars_kwargs_mapping,\n",
    "            output_variables=output_variables,\n",
    "        ),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def ray_chain_fn(data, chain, block_size=1500, num_cpus=8, num_gpus=1):\n",
    "    \"\"\"\n",
    "    This function is used to run a given chain function on a Ray cluster.\n",
    "    It splits the input data into blocks, and runs the chain function on each block in parallel.\n",
    "    The results are then combined and returned.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # If the block size is None, run the chain function on the entire dataset.\n",
    "        if block_size is None:\n",
    "            return chain(data)\n",
    "\n",
    "        # Convert the data into a dictionary of lists.\n",
    "        ray_data = {k: v for k, v in data.items()}\n",
    "        for k, v in ray_data.items():\n",
    "            if not is_list(v):\n",
    "                ray_data[k] = [v]\n",
    "\n",
    "        # Get the maximum length of any list in the data dictionary.\n",
    "        max_len = max([len(v) for v in ray_data.values()])\n",
    "\n",
    "        # If the maximum length is 1 or less than the block size, run the chain function on the entire dataset.\n",
    "        if max_len == 1 or max_len <= block_size:\n",
    "            return chain(data)\n",
    "\n",
    "        # If the maximum length is greater than the block size, split the data into blocks.\n",
    "        for k, v in ray_data.items():\n",
    "            if len(v) < max_len:\n",
    "                ray_data[k] = v * max_len\n",
    "\n",
    "        # Convert the data dictionary into a Pandas DataFrame.\n",
    "        df = pd.DataFrame(ray_data)\n",
    "\n",
    "        # Initialize Ray.\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Calculate the number of blocks to split the data into.\n",
    "        num_blocks = int(np.ceil(len(df) / block_size))\n",
    "        msg.info(f\"Running chain on {num_blocks} blocks.\", spaced=True)\n",
    "\n",
    "        # Calculate the number of CPUs and GPUs to use per block.\n",
    "        num_cpus = min(ray.available_resources()[\"CPU\"], num_cpus)\n",
    "        num_cpus /= num_blocks\n",
    "        if num_gpus is not None:\n",
    "            num_gpus = min(ray.available_resources().get(\"GPU\", 0), num_gpus)\n",
    "            if num_gpus > 0:\n",
    "                num_gpus /= num_blocks\n",
    "                num_cpus = None\n",
    "            else:\n",
    "                num_gpus = None\n",
    "        # Convert the Pandas DataFrame into a Ray dataset and run the chain function on each block.\n",
    "        ds = rd.from_pandas(df).repartition(num_blocks)\n",
    "        res = (\n",
    "            ds.map_batches(\n",
    "                lambda x: chain(x),\n",
    "                batch_size=block_size,\n",
    "                num_cpus=num_cpus,\n",
    "                num_gpus=num_gpus,\n",
    "            )\n",
    "            .to_pandas()\n",
    "            .to_dict(orient=\"list\")\n",
    "        )\n",
    "\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"Error in ray_chain_fn: {e}\", spaced=True)\n",
    "        return chain(data)\n",
    "\n",
    "\n",
    "def ray_chain(chain, block_size=1500, num_cpus=8, num_gpus=1, verbose=False):\n",
    "    \"\"\"\n",
    "    This function is used to run a given chain function on a Ray cluster if necessary.\n",
    "    \"\"\"\n",
    "    tfm = partial(\n",
    "        ray_chain_fn,\n",
    "        chain=chain,\n",
    "        block_size=block_size,\n",
    "        num_cpus=num_cpus,\n",
    "        num_gpus=num_gpus,\n",
    "    )\n",
    "    input_variables = chain.input_keys\n",
    "    output_variables = chain.output_keys\n",
    "    return TransformChain(\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        transform=tfm,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def noop_chain():\n",
    "    return transform_chain(noop)\n",
    "\n",
    "\n",
    "def docs_to_json_chain(\n",
    "    json_folder,\n",
    "    data={},\n",
    "    data_key=\"data\",\n",
    "    with_metadata=True,\n",
    "    with_content=True,\n",
    "    indent=None,\n",
    "    input_variables=[\"docs\"],\n",
    "    output_variables=[\"docs\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    return transform_chain(\n",
    "        docs_to_json,\n",
    "        transform_kwargs=dict(\n",
    "            json_folder=json_folder,\n",
    "            data=data,\n",
    "            data_key=data_key,\n",
    "            with_metadata=with_metadata,\n",
    "            with_content=with_content,\n",
    "            indent=indent,\n",
    "        ),\n",
    "        vars_kwargs_mapping={input_variables[0]: \"docs\"},\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    \n",
    "def add_str_to_docs_chain(\n",
    "    str,\n",
    "    input_variables=[\"docs\"],\n",
    "    output_variables=[\"cat_docs\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Chain that adds a string to the docs\"\n",
    "    return transform_chain(\n",
    "        add_str_to_docs,\n",
    "        transform_kwargs=dict(str=str),\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        vars_kwargs_mapping={input_variables[0]: \"docs\"},\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
