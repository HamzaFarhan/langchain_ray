{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ner.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langchain_ray.imports import *\n",
    "from langchain_ray.chains import *\n",
    "from langchain_ray.utils import *\n",
    "from langchain_ray.pdf.utils import *\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class EduNER(BaseModel):\n",
    "    degree: str = Field(description=\"The degree of the education.\", default=None)\n",
    "    major: str = Field(description=\"The major of the education.\", default=None)\n",
    "    school: str = Field(description=\"The school of the education.\", default=None)\n",
    "    date: str = Field(description=\"The date of the education.\", default=None)\n",
    "\n",
    "\n",
    "class JobNER(BaseModel):\n",
    "    role: str = Field(description=\"The role of the job.\", default=None)\n",
    "    company: str = Field(description=\"The company of the job.\", default=None)\n",
    "    duration: str = Field(description=\"The duration of the job.\", default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_edu_model(model_name=\"tner/deberta-v3-large-ontonotes5\", device=\"cpu\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    return pipeline(\n",
    "        \"ner\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        aggregation_strategy=\"simple\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_job_model(model_name=\"ismail-lucifer011/autotrain-job_all-903929564\", device=\"cpu\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    return pipeline(\n",
    "        \"ner\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        aggregation_strategy=\"simple\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "def proc_ners(ners, ner_dict={\"institute\": \"\", \"date\": \"\"}, thresh=3):\n",
    "    ner_dict2 = copy.deepcopy(ner_dict)\n",
    "    org_key = \"institute\" if \"institute\" in ner_dict2 else \"company\"\n",
    "    mapper = {\n",
    "        \"ORG\": org_key,\n",
    "        \"FAC\": org_key,\n",
    "        \"GPE\": org_key,\n",
    "        \"LOC\": org_key,\n",
    "        \"Job\": \"role\",\n",
    "        \"WORK_OF_ART\": \"degree\",\n",
    "        \"DATE\": \"date\",\n",
    "    }\n",
    "    ner_dicts = []\n",
    "    for ner in ners:\n",
    "        if len(ner) == 0:\n",
    "            ner_dicts.append({})\n",
    "            continue\n",
    "        try:\n",
    "            for d in ner:\n",
    "                eg = d[\"entity_group\"]\n",
    "                w = \" \" + d[\"word\"].strip()\n",
    "                k = mapper.get(eg, None)\n",
    "                if k is not None and ner_dict2.get(k, None) is not None and not w.startswith(\"##\"):\n",
    "                    ner_dict2[k] = (ner_dict2[k] + w).strip()\n",
    "            res = {k: v for k, v in ner_dict2.items() if len(v) > thresh}\n",
    "            if res.get(org_key, None) is not None:\n",
    "                ner_dicts.append(res)\n",
    "            else:\n",
    "                ner_dicts.append({})\n",
    "            ner_dict2 = copy.deepcopy(ner_dict)\n",
    "        except Exception as e:\n",
    "            msg.fail(f\"proc_ners failed with error: {e}\", spaced=True)\n",
    "            ner_dicts.append({})\n",
    "            ner_dict2 = copy.deepcopy(ner_dict)\n",
    "    return ner_dicts\n",
    "\n",
    "\n",
    "def job_ner(docs, e_ner, j_ner):\n",
    "    return j_ner(docs), e_ner(docs)\n",
    "\n",
    "\n",
    "def edu_ner(docs, e_ner, ner_dict={\"institute\": \"\", \"date\": \"\"}):\n",
    "    ners = e_ner(docs)\n",
    "    return proc_ners(ners, ner_dict)\n",
    "\n",
    "\n",
    "def work_ner(docs, e_ner, j_ner, ner_dict={\"company\": \"\", \"date\": \"\"}):\n",
    "    ner1, ner2 = job_ner(docs, e_ner, j_ner)\n",
    "    ners = [n1 + n2 for n1, n2 in zip(ner1, ner2)]\n",
    "    return proc_ners(ners, ner_dict)\n",
    "\n",
    "\n",
    "def docs_to_ners(docs, e_ner, j_ner):\n",
    "    ners = [{}] * len(docs)\n",
    "    work_docs = np.array(\n",
    "        [\n",
    "            [i, doc.page_content]\n",
    "            for i, doc in enumerate(docs)\n",
    "            if doc.metadata.get(\"category\", None) == \"Work Experience\"\n",
    "        ]\n",
    "    )\n",
    "    work_docs_idx = work_docs[:, 0].astype(int)\n",
    "    work_docs = work_docs[:, 1].tolist()\n",
    "    try:\n",
    "        work_ners = work_ner(work_docs, e_ner, j_ner)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"work_ner failed with error: {e}\", spaced=True)\n",
    "        work_ners = [{}] * len(work_docs)\n",
    "    for i, doc in enumerate(work_docs_idx):\n",
    "        ners[doc] = work_ners[i]\n",
    "    edu_docs = np.array(\n",
    "        [\n",
    "            [i, doc.page_content]\n",
    "            for i, doc in enumerate(docs)\n",
    "            if doc.metadata.get(\"category\", None) == \"Education\"\n",
    "        ]\n",
    "    )\n",
    "    edu_docs_idx = edu_docs[:, 0].astype(int)\n",
    "    edu_docs = edu_docs[:, 1].tolist()\n",
    "    try:\n",
    "        edu_ners = edu_ner(edu_docs, e_ner)\n",
    "    except Exception as e:\n",
    "        msg.fail(f\"edu_ner failed with error: {e}\", spaced=True)\n",
    "        edu_ners = [{}] * len(edu_docs)\n",
    "    for i, doc in enumerate(edu_docs_idx):\n",
    "        ners[doc] = edu_ners[i]\n",
    "    return ners\n",
    "\n",
    "def add_ners_to_docs(docs, e_ner, j_ner, key=\"ner\"):\n",
    "    fn = partial(docs_to_ners, e_ner=e_ner, j_ner=j_ner)\n",
    "    return add_docs_metadata(docs, fn, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# #| eval: false\n",
    "\n",
    "\n",
    "# examples = [{\"text\": \"text 1\", \"ner\": \"ner 1\"}, {\"text\": \"text 2\", \"ner\": \"ner 2\"}]\n",
    "\n",
    "# job_parser = PydanticOutputParser(pydantic_object=JobNER)\n",
    "# ex_temp = \"text: {text}\\n{ner}\"\n",
    "# ex_prompt = PromptTemplate.from_template(ex_temp)\n",
    "# ex_prompt = FewShotPromptTemplate(\n",
    "#     examples=examples,\n",
    "#     example_prompt=ex_prompt,\n",
    "#     prefix=\"Extract ner from this text:\\n{format_instructions}\\nHere are some examples:\\n\",\n",
    "#     suffix=\"Now it's your turn:\\ntext: {text}\",\n",
    "#     input_variables=[\"format_instructions\", \"text\"],\n",
    "# ).partial(format_instructions=job_parser.get_format_instructions())\n",
    "# #| hide\n",
    "# #| eval: false\n",
    "\n",
    "\n",
    "# tt_pipe = HuggingFacePipeline(\n",
    "#     pipeline=pipeline(\n",
    "#         \"text2text-generation\",\n",
    "#         model=\"google/flan-t5-large\",\n",
    "#         device_map=default_device(),\n",
    "#         max_new_tokens=256,\n",
    "#     )\n",
    "# )\n",
    "# #| hide\n",
    "# #| eval: false\n",
    "\n",
    "# gen_chain = LLMChain(prompt=ex_prompt, llm=tt_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
